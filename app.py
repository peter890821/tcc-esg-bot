import io
import os
import re
import tempfile
from dataclasses import dataclass
from typing import List, Optional, Tuple

import pandas as pd
import streamlit as st
from docx import Document
from pptx import Presentation
from pypdf import PdfReader

try:
    import google.generativeai as genai
except Exception:  # pragma: no cover
    genai = None

try:
    import chromadb
    from chromadb.utils import embedding_functions
except Exception:  # pragma: no cover
    chromadb = None

try:
    from unstructured.partition.pdf import partition_pdf
    from bs4 import BeautifulSoup
except Exception:  # pragma: no cover
    partition_pdf = None
    BeautifulSoup = None


DEFAULT_CSV_PATH = "MSCI_Methodology_Full_KB.csv"
KB_DIRECTORY = "kb"  # å°ˆé–€æ”¾é•·æœŸçŸ¥è­˜åº«æª”æ¡ˆçš„è³‡æ–™å¤¾
REQUIRED_COLUMNS = {"text_content", "source_file", "doc_type"}


@dataclass
class RetrievedChunk:
    text_content: str
    source_file: str
    doc_type: str
    score: int
    year: Optional[int] = None
    version: Optional[str] = None
    page_number: Optional[int] = None


def _safe_str(x) -> str:
    return "" if x is None else str(x)


def sanitize_text(text: str) -> str:
    """
    æ¸…æ´—å­—ä¸²ï¼Œç§»é™¤ç„¡æ³•è¢« UTF-8 ç·¨ç¢¼çš„ surrogate å­—å…ƒã€‚
    é€™é€šå¸¸ç™¼ç”Ÿåœ¨è™•ç†è¤‡é›œ PDF æå–å‡ºçš„æå£å­—å…ƒã€‚
    """
    if not isinstance(text, str):
        return str(text)
    # ä½¿ç”¨ 'ignore' æ‹‹æ£„ç„¡æ³•ç·¨ç¢¼çš„å­—å…ƒï¼Œå†è§£ç¢¼å›ä¾†
    return text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')


def tokenize_question(question: str) -> List[str]:
    """
    Very simple tokenization:
    - Extract alphanumeric "words" (English/numbers)
    - Extract CJK sequences (Chinese/Japanese/Korean characters)
    """
    q = question.strip().lower()
    if not q:
        return []

    tokens: List[str] = []
    tokens += re.findall(r"[a-z0-9]+", q)
    tokens += re.findall(r"[\u4e00-\u9fff]+", q)

    # Keep unique tokens, preserve order
    seen = set()
    deduped = []
    for t in tokens:
        if t and t not in seen:
            seen.add(t)
            deduped.append(t)
    return deduped


def simple_retrieve_topk(df: pd.DataFrame, question: str, k: int = 5) -> List[RetrievedChunk]:
    """
    Simple string matching retrieval (fallback when vector search unavailable):
    score = sum(count(token in text_content)) + (bonus if full question is substring)
    """
    tokens = tokenize_question(question)
    if not tokens:
        return []

    q_lower = question.strip().lower()

    chunks: List[RetrievedChunk] = []
    for idx, row in df.iterrows():
        text = _safe_str(row.get("text_content"))
        if not text:
            continue
        text_lower = text.lower()

        score = 0
        for t in tokens:
            score += text_lower.count(t)
        if q_lower and q_lower in text_lower:
            score += 5

        if score > 0:
            chunks.append(
                RetrievedChunk(
                    text_content=text,
                    source_file=_safe_str(row.get("source_file")),
                    doc_type=_safe_str(row.get("doc_type")),
                    score=score,
                    year=row.get("year") if pd.notna(row.get("year")) else None,
                    version=_safe_str(row.get("version")) if row.get("version") else None,
                    page_number=row.get("page_number") if pd.notna(row.get("page_number")) else None,
                )
            )

    chunks.sort(key=lambda c: c.score, reverse=True)
    return chunks[:k]


@st.cache_resource
def init_vector_db(df: pd.DataFrame, api_key: str) -> Optional[object]:
    """
    åˆå§‹åŒ–å‘é‡è³‡æ–™åº«ï¼Œå°‡çŸ¥è­˜åº«åµŒå…¥
    ä½¿ç”¨ ChromaDB + Gemini Embeddings
    """
    if chromadb is None:
        st.warning("ChromaDB æœªå®‰è£ï¼Œå°‡ä½¿ç”¨ç°¡æ˜“å­—ä¸²æª¢ç´¢ã€‚å»ºè­°åŸ·è¡Œï¼špip install chromadb")
        return None
    
    try:
        client = chromadb.Client()
        
        # ä½¿ç”¨ Gemini Embedding API
        gemini_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(
            api_key=api_key,
            model_name="models/text-embedding-004"
        )
        
        # å»ºç«‹ collectionï¼ˆè‹¥å·²å­˜åœ¨å‰‡åˆªé™¤é‡å»ºï¼‰
        try:
            client.delete_collection(name="tcc_esg_kb")
        except:
            pass
        
        collection = client.create_collection(
            name="tcc_esg_kb",
            embedding_function=gemini_ef,
            metadata={"hnsw:space": "cosine"}
        )
        
        # æ‰¹é‡åµŒå…¥æ–‡æª”
        valid_texts = []
        valid_metadatas = []
        valid_ids = []
        
        for idx, row in df.iterrows():
            text = _safe_str(row.get("text_content"))
            if text.strip():  # åªåŠ å…¥éç©ºæ–‡æœ¬
                valid_texts.append(text)
                valid_ids.append(f"chunk_{idx}")
                valid_metadatas.append({
                    "source_file": _safe_str(row.get("source_file")),
                    "doc_type": _safe_str(row.get("doc_type")),
                    "year": int(row.get("year")) if pd.notna(row.get("year")) else 0,
                    "idx": int(idx)
                })
        
        if valid_texts:
            collection.add(
                ids=valid_ids,
                documents=valid_texts,
                metadatas=valid_metadatas
            )
        
        return collection
    
    except Exception as e:
        st.error(f"å‘é‡è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—ï¼š{e}ã€‚å°‡ä½¿ç”¨ç°¡æ˜“æª¢ç´¢ã€‚")
        return None


def extract_chinese_terms(text: str) -> List[str]:
    """æå–ä¸­æ–‡é‡è¦è¡“èªï¼ˆ3å­—ä»¥ä¸Šï¼‰"""
    terms = re.findall(r'[\u4e00-\u9fff]{3,}', text)
    return list(set(terms))


def hybrid_retrieve_topk(
    collection: Optional[object],
    df: pd.DataFrame,
    question: str,
    k: int = 5
) -> List[RetrievedChunk]:
    """
    æ··åˆæª¢ç´¢ï¼šå‘é‡èªç¾©æœå°‹ (70%) + é—œéµå­—ç²¾ç¢ºåŒ¹é… (30%)
    é©ç”¨æ–¼ ESG é ˜åŸŸçš„å°ˆæ¥­è¡“èªæª¢ç´¢
    """
    # Fallback to simple search if vector DB unavailable
    if collection is None:
        return simple_retrieve_topk(df, question, k)
    
    try:
        # 1. å‘é‡æœå°‹ï¼ˆèªç¾©ç†è§£ï¼‰
        results = collection.query(
            query_texts=[question],
            n_results=min(k * 2, len(df))  # å–é›™å€å€™é¸
        )
        
        vector_scores = {}
        if results['ids'] and results['ids'][0]:
            for i, chunk_id in enumerate(results['ids'][0]):
                idx = int(chunk_id.split('_')[1])
                # è·é›¢è½‰ç‚ºç›¸ä¼¼åº¦åˆ†æ•¸ (0-1)
                distance = results['distances'][0][i]
                similarity = max(0, 1.0 - distance)
                vector_scores[idx] = similarity
        
        # 2. é—œéµå­—ç²¾ç¢ºåŒ¹é…ï¼ˆå°ˆæœ‰åè©ï¼‰
        keyword_patterns = [
            r'ISO\s*\d+',
            r'IFRS\s*S\d+',
            r'GRI\s*\d+',
            r'Scope\s*[123]',
            r'TCFD',
            r'SASB',
            r'CBAM',
            r'MSCI'
        ]
        
        keyword_scores = {}
        q_lower = question.lower()
        important_terms = extract_chinese_terms(question)
        
        for idx, row in df.iterrows():
            text = _safe_str(row.get("text_content"))
            if not text:
                continue
            
            text_lower = text.lower()
            keyword_score = 0.0
            
            # æª¢æŸ¥å°ˆæœ‰åè© (é«˜æ¬Šé‡)
            for pattern in keyword_patterns:
                if re.search(pattern, question, re.IGNORECASE):
                    matches = len(re.findall(pattern, text, re.IGNORECASE))
                    keyword_score += matches * 0.3
            
            # æª¢æŸ¥ä¸­æ–‡é‡è¦è¡“èª (ä¸­æ¬Šé‡)
            for term in important_terms:
                if term in text:
                    keyword_score += text.count(term) * 0.2
            
            # æª¢æŸ¥å®Œæ•´å•é¡ŒåŒ¹é… (é«˜æ¬Šé‡)
            if len(q_lower) > 5 and q_lower in text_lower:
                keyword_score += 0.5
            
            if keyword_score > 0:
                keyword_scores[idx] = min(keyword_score, 1.0)  # é™åˆ¶æœ€å¤§å€¼ç‚º1
        
        # 3. æ··åˆæ’åºï¼ˆå‘é‡ 70% + é—œéµå­— 30%ï¼‰
        combined_scores = {}
        
        # åˆä½µæ‰€æœ‰å€™é¸
        all_indices = set(list(vector_scores.keys()) + list(keyword_scores.keys()))
        
        for idx in all_indices:
            vector_score = vector_scores.get(idx, 0.0)
            keyword_score = keyword_scores.get(idx, 0.0)
            combined_scores[idx] = vector_score * 0.7 + keyword_score * 0.3
        
        # æ’åºä¸¦å– Top-K
        sorted_indices = sorted(
            combined_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:k]
        
        # æ§‹å»ºçµæœ
        results = []
        for idx, score in sorted_indices:
            row = df.iloc[idx]
            results.append(
                RetrievedChunk(
                    text_content=_safe_str(row.get("text_content")),
                    source_file=_safe_str(row.get("source_file")),
                    doc_type=_safe_str(row.get("doc_type")),
                    score=int(score * 100),  # è½‰ç‚ºç™¾åˆ†æ¯”
                    year=int(row.get("year")) if pd.notna(row.get("year")) else None,
                    version=_safe_str(row.get("version")) if row.get("version") else None,
                    page_number=int(row.get("page_number")) if pd.notna(row.get("page_number")) else None,
                )
            )
        
        return results
    
    except Exception as e:
        st.warning(f"å‘é‡æª¢ç´¢å¤±æ•—ï¼š{e}ã€‚åˆ‡æ›è‡³ç°¡æ˜“æª¢ç´¢ã€‚")
        return simple_retrieve_topk(df, question, k)


def html_table_to_markdown(html: str) -> str:
    """å°‡ HTML è¡¨æ ¼è½‰ç‚º Markdownï¼ˆä¿ç•™çµæ§‹ï¼ŒAI æ›´æ˜“ç†è§£ï¼‰"""
    if BeautifulSoup is None:
        return html
    
    try:
        soup = BeautifulSoup(html, 'html.parser')
        table = soup.find('table')
        
        if not table:
            return html
        
        rows = table.find_all('tr')
        if not rows:
            return html
        
        md_lines = []
        
        for i, row in enumerate(rows):
            cells = row.find_all(['th', 'td'])
            if not cells:
                continue
            
            # æå–å–®å…ƒæ ¼æ–‡å­—
            cell_texts = [c.get_text(strip=True) for c in cells]
            md_lines.append('| ' + ' | '.join(cell_texts) + ' |')
            
            # ç¬¬ä¸€è¡Œå¾Œæ·»åŠ è¡¨é ­åˆ†éš”ç·š
            if i == 0:
                md_lines.append('| ' + ' | '.join(['---'] * len(cells)) + ' |')
        
        return '\n'.join(md_lines)
    
    except Exception:
        return html


def _extract_text_from_pdf(file_bytes: bytes, filename: str = "document.pdf") -> str:
    """
    ä½¿ç”¨ Unstructured è§£æ PDFï¼Œä¿ç•™è¡¨æ ¼çµæ§‹
    è‹¥ Unstructured ä¸å¯ç”¨ï¼Œå›é€€è‡³åŸºæœ¬è§£æ
    """
    # å˜—è©¦ä½¿ç”¨ Unstructuredï¼ˆè¡¨æ ¼æ„ŸçŸ¥ï¼‰
    if partition_pdf is not None:
        try:
            # æš«å­˜æª”æ¡ˆï¼ˆUnstructured éœ€è¦æª”æ¡ˆè·¯å¾‘ï¼‰
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:
                tmp.write(file_bytes)
                tmp_path = tmp.name
            
            try:
                # åˆ†å€è§£æï¼ˆè‡ªå‹•æª¢æ¸¬è¡¨æ ¼ï¼‰
                elements = partition_pdf(
                    filename=tmp_path,
                    strategy="hi_res",  # é«˜è§£æåº¦ï¼Œå•Ÿç”¨è¡¨æ ¼æª¢æ¸¬
                    infer_table_structure=True,
                    extract_images_in_pdf=False
                )
                
                # åˆ†é¡è™•ç†
                text_parts = []
                for elem in elements:
                    if elem.category == "Table":
                        # è¡¨æ ¼è½‰ Markdown æ ¼å¼
                        table_html = elem.metadata.text_as_html if hasattr(elem.metadata, 'text_as_html') else str(elem)
                        table_md = html_table_to_markdown(table_html)
                        text_parts.append(f"\n[è¡¨æ ¼]\n{table_md}\n")
                    else:
                        text_parts.append(elem.text)
                
                return sanitize_text("\n".join(text_parts))
            
            finally:
                # æ¸…ç†æš«å­˜æª”
                try:
                    os.unlink(tmp_path)
                except:
                    pass
        
        except Exception as e:
            # Unstructured å¤±æ•—ï¼Œå›é€€è‡³åŸºæœ¬è§£æ
            st.warning(f"é«˜ç´š PDF è§£æå¤±æ•—ï¼ˆ{filename}ï¼‰ï¼š{e}ã€‚ä½¿ç”¨åŸºæœ¬è§£æã€‚")
    
    # åŸºæœ¬è§£æï¼ˆFallbackï¼‰
    reader = PdfReader(io.BytesIO(file_bytes))
    texts: List[str] = []
    for page in reader.pages:
        try:
            page_text = page.extract_text() or ""
        except Exception:
            page_text = ""
        if page_text:
            texts.append(page_text)
    return sanitize_text("\n\n".join(texts))


def auto_extract_year(filename: str, content: str = "") -> Optional[int]:
    """
    å¾æª”åæˆ–å…§å®¹è‡ªå‹•æå–å¹´ä»½
    
    æ”¯æ´æ ¼å¼ï¼š
    - è¥¿å…ƒå¹´ï¼š2024, 2023
    - æ°‘åœ‹å¹´ï¼šæ°‘åœ‹ 113 å¹´ â†’ 2024
    - ISO æ—¥æœŸï¼š2024-01-15
    """
    # 1. æª”åå„ªå…ˆï¼ˆä¾‹å¦‚ï¼šTCC_ESG_Report_2024.pdfï¼‰
    year_match = re.search(r'20\d{2}', filename)
    if year_match:
        year = int(year_match.group())
        # é©—è­‰åˆç†æ€§ï¼ˆ2000-2050ï¼‰
        if 2000 <= year <= 2050:
            return year
    
    # 2. å…§å®¹ä¸­æŸ¥æ‰¾ï¼ˆåªæª¢æŸ¥å‰ 2000 å­—å…ƒï¼‰
    if content:
        sample = content[:2000]
        
        # æ°‘åœ‹å¹´ï¼ˆä¾‹å¦‚ï¼šã€Œæ°‘åœ‹ 113 å¹´ã€ â†’ 2024ï¼‰
        roc_match = re.search(r'æ°‘åœ‹\s*(\d{3})', sample)
        if roc_match:
            roc_year = int(roc_match.group(1))
            if 100 <= roc_year <= 150:  # åˆç†ç¯„åœ
                return roc_year + 1911
        
        # ISO æ—¥æœŸæ ¼å¼ï¼ˆä¾‹å¦‚ï¼š2024-01-15ï¼‰
        iso_match = re.search(r'20\d{2}-\d{2}-\d{2}', sample)
        if iso_match:
            year = int(iso_match.group()[:4])
            if 2000 <= year <= 2050:
                return year
        
        # ç´”è¥¿å…ƒå¹´ï¼ˆä¾‹å¦‚ï¼šã€Œ2024 å¹´åº¦å ±å‘Šã€ï¼‰
        year_match = re.search(r'(20\d{2})\s*å¹´', sample)
        if year_match:
            year = int(year_match.group(1))
            if 2000 <= year <= 2050:
                return year
    
    return None


def extract_version(filename: str) -> Optional[str]:
    """
    å¾æª”åæå–ç‰ˆæœ¬è™Ÿ
    
    æ”¯æ´æ ¼å¼ï¼š
    - v1.0, V2.3
    - version_1.0
    - _final, _draft
    """
    # ç‰ˆæœ¬è™Ÿæ ¼å¼ï¼ˆv1.0, V2.3ï¼‰
    version_match = re.search(r'[vV](\d+\.\d+)', filename)
    if version_match:
        return f"v{version_match.group(1)}"
    
    # version_xxx
    version_match = re.search(r'version[_\s]*(\d+(?:\.\d+)?)', filename, re.IGNORECASE)
    if version_match:
        return f"v{version_match.group(1)}"
    
    # ç‹€æ…‹æ¨™è¨˜
    if '_final' in filename.lower() or '-final' in filename.lower():
        return "final"
    if '_draft' in filename.lower() or '-draft' in filename.lower():
        return "draft"
    
    return None


def _extract_text_from_docx(file_bytes: bytes) -> str:
    doc = Document(io.BytesIO(file_bytes))
    paras = [p.text for p in doc.paragraphs if p.text]
    return sanitize_text("\n".join(paras))


def _extract_text_from_pptx(file_bytes: bytes) -> str:
    pres = Presentation(io.BytesIO(file_bytes))
    texts: List[str] = []
    for slide in pres.slides:
        for shape in slide.shapes:
            if hasattr(shape, "text") and shape.text:
                texts.append(shape.text)
    return sanitize_text("\n\n".join(texts))


@st.cache_data(show_spinner=False)
def load_kb_from_bytes(file_bytes: bytes, filename: str) -> pd.DataFrame:
    """
    Load uploaded knowledge base file into a standardized DataFrame
    with columns: text_content, source_file, doc_type, year, version.

    æ”¯æ´é¡å‹ï¼šCSVã€Excelã€PDFã€Wordã€PPTã€‚
    - å°æ–¼é CSV/Excelï¼Œæˆ‘å€‘æœƒæŠŠæ•´å€‹æª”æ¡ˆæ–‡å­—ç•¶æˆä¸€å€‹æ®µè½ã€‚
    - è‡ªå‹•æå–å¹´ä»½èˆ‡ç‰ˆæœ¬ metadata
    """
    name = filename or "uploaded_file"
    lower = name.lower()
    ext = ""
    if "." in lower:
        ext = lower.rsplit(".", 1)[-1]

    buffer = io.BytesIO(file_bytes)

    # Structured: CSV / Excel
    if ext in {"csv", "xlsx"}:
        if ext == "xlsx":
            table = pd.read_excel(buffer)
        else:
            table = pd.read_csv(buffer)

        cols = set(str(c) for c in table.columns)
        # è‹¥å·²ç¶“æœ‰æ¨™æº–æ¬„ä½ï¼Œå°±ç›´æ¥ä½¿ç”¨
        if REQUIRED_COLUMNS.issubset(cols):
            df = table.copy()
            for col in REQUIRED_COLUMNS:
                df[col] = df[col].fillna("").astype(str)
            
            # è£œå…… metadata æ¬„ä½ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
            if "year" not in df.columns:
                df["year"] = auto_extract_year(name, "")
            if "version" not in df.columns:
                df["version"] = extract_version(name)
            
            return df[["text_content", "source_file", "doc_type", "year", "version"]]

        # å¦å‰‡ï¼šæŠŠæ•´å¼µè¡¨è½‰æˆä¸€æ®µæ–‡å­—
        text_repr = table.astype(str).to_csv(index=False)
        return pd.DataFrame(
            [
                {
                    "text_content": text_repr,
                    "source_file": name,
                    "doc_type": ext or "table",
                    "year": auto_extract_year(name, text_repr),
                    "version": extract_version(name),
                }
            ]
        )

    # Unstructured: PDF / DOCX / PPTX
    if ext == "pdf":
        text = _extract_text_from_pdf(file_bytes, filename=name)
    elif ext in {"docx", "doc"}:
        text = _extract_text_from_docx(file_bytes)
    elif ext in {"pptx", "ppt"}:
        text = _extract_text_from_pptx(file_bytes)
    else:
        # Fallbackï¼šç•¶ä½œä¸€èˆ¬æ–‡å­—æª”
        try:
            text = buffer.read().decode("utf-8")
        except Exception:
            text = ""

    return pd.DataFrame(
        [
            {
                "text_content": text,
                "source_file": name,
                "doc_type": ext or "file",
                "year": auto_extract_year(name, text),
                "version": extract_version(name),
            }
        ]
    )


@st.cache_data(show_spinner=False)
def load_csv_from_path(path: str) -> pd.DataFrame:
    return pd.read_csv(path)


def get_kb_dataframe(uploaded_files) -> Tuple[Optional[pd.DataFrame], Optional[str]]:
    """
    Returns (df, error_message). error_message is None when success.

    çŸ¥è­˜ä¾†æºå„ªå…ˆé †åºï¼š
    1. å°ˆæ¡ˆå…§çš„ kb/ è³‡æ–™å¤¾ï¼ˆé•·æœŸçŸ¥è­˜åº«ï¼Œå•Ÿå‹•æ™‚è‡ªå‹•è¼‰å…¥ï¼‰
    2. ä½¿ç”¨è€…åœ¨ç¶²é ä¸Šè‡¨æ™‚ä¸Šå‚³çš„æª”æ¡ˆ
    3. å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹çš„é è¨­ CSVï¼šMSCI_Methodology_Full_KB.csv
    """
    dfs: List[pd.DataFrame] = []

    # 1) æƒææœ¬åœ° kb/ è³‡æ–™å¤¾ï¼ˆéè¿´æƒææ‰€æœ‰å­è³‡æ–™å¤¾ï¼‰
    if os.path.isdir(KB_DIRECTORY):
        for root, _, files in os.walk(KB_DIRECTORY):
            for name in files:
                lower = name.lower()
                if not lower.endswith((".csv", ".xlsx", ".pdf", ".docx", ".pptx")):
                    continue
                path = os.path.join(root, name)
                # ä¿ç•™ç›¸å°æ–¼ kb/ çš„å®Œæ•´è·¯å¾‘ï¼ˆåŒ…å«å­è³‡æ–™å¤¾ï¼‰ï¼Œä¾‹å¦‚ï¼šç’°å¢ƒç›¸é—œ/report.pdf
                rel_path = os.path.relpath(path, KB_DIRECTORY)
                try:
                    with open(path, "rb") as f:
                        file_bytes = f.read()
                    df_part = load_kb_from_bytes(file_bytes, rel_path)
                    dfs.append(df_part)
                except Exception:
                    # è‹¥å–®ä¸€æª”æ¡ˆå¤±æ•—ï¼Œä¸å½±éŸ¿æ•´é«”ï¼Œå¯åŠ æ—¥å¾Œ logging
                    continue

    # 2) ç¶²é ä¸Šè‡¨æ™‚ä¸Šå‚³çš„æª”æ¡ˆ
    if uploaded_files:
        try:
            for f in uploaded_files:
                file_bytes = f.getvalue()
                df_part = load_kb_from_bytes(file_bytes, f.name)
                dfs.append(df_part)
        except Exception as e:
            return None, f"è®€å–ä¸Šå‚³æª”æ¡ˆå¤±æ•—ï¼š{e}"

    # 3) è‹¥å‰å…©è€…éƒ½æ²’æœ‰è³‡æ–™ï¼Œè©¦è‘—è¼‰å…¥é è¨­ CSV
    if not dfs:
        try:
            df = load_csv_from_path(DEFAULT_CSV_PATH)
        except FileNotFoundError:
            return None, (
                f"æ‰¾ä¸åˆ°ä»»ä½•çŸ¥è­˜åº«è³‡æ–™ã€‚\n"
                f"- è‹¥è¦ä½¿ç”¨é è¨­æª”æ¡ˆï¼Œè«‹å°‡ {DEFAULT_CSV_PATH} æ”¾åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ã€‚\n"
                f"- æˆ–å»ºç«‹ `{KB_DIRECTORY}` è³‡æ–™å¤¾ï¼Œæ”¾å…¥ CSV/Excel/PDF/Word/PPT æª”æ¡ˆã€‚\n"
                f"- æˆ–ç›´æ¥åœ¨å·¦å´ä¸Šå‚³ ESG çŸ¥è­˜åº«æª”æ¡ˆã€‚"
            )
        except pd.errors.EmptyDataError:
            return None, "é è¨­ CSV æª”æ¡ˆæ˜¯ç©ºçš„æˆ–æ ¼å¼ä¸æ­£ç¢ºã€‚"
        except UnicodeDecodeError:
            return None, "é è¨­ CSV ç·¨ç¢¼è®€å–å¤±æ•—ã€‚è«‹å˜—è©¦å¦å­˜ç‚º UTF-8 å¾Œé‡æ–°æ”¾ç½®ã€‚"
        except Exception as e:
            return None, f"è®€å–é è¨­ CSV å¤±æ•—ï¼š{e}"

        missing = REQUIRED_COLUMNS - set(df.columns)
        if missing:
            return None, f"é è¨­ CSV ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{', '.join(sorted(missing))}ã€‚éœ€è¦æ¬„ä½ï¼š{', '.join(sorted(REQUIRED_COLUMNS))}"

        for col in REQUIRED_COLUMNS:
            df[col] = df[col].fillna("").astype(str)

        return df[list(REQUIRED_COLUMNS)], None

    # åˆä½µä¾†è‡ª kb/ èˆ‡ä¸Šå‚³çš„æ‰€æœ‰è³‡æ–™
    df_all = pd.concat(dfs, ignore_index=True)
    for col in REQUIRED_COLUMNS:
        if col not in df_all.columns:
            df_all[col] = ""
        df_all[col] = df_all[col].fillna("").astype(str)

    return df_all[list(REQUIRED_COLUMNS)], None


def build_prompt(context: str, question: str) -> str:
    """
    æ§‹å»ºå°ˆæ¥­ ESG é¡§å• Promptï¼Œæ•´åˆæ¡†æ¶å°ç…§èˆ‡çµæ§‹åŒ–è¼¸å‡º
    """
    # æ¸…æ´—è¼¸å…¥ï¼Œç¢ºä¿æ²’æœ‰ surrogate å­—å…ƒ
    clean_context = sanitize_text(context)
    clean_question = sanitize_text(question)
    return (
        "ä½ ç¾åœ¨æ˜¯å°æ³¥é›†åœ˜ (TCC) çš„é¦–å¸­æ°¸çºŒç­–ç•¥é¡§å•èˆ‡ ESG åˆ†æå°ˆå®¶ã€‚\n\n"
        
        "ã€æ ¸å¿ƒä»»å‹™ã€‘\n"
        f"æ ¹æ“šä»¥ä¸‹èƒŒæ™¯è³‡æ–™å›ç­”ä½¿ç”¨è€…å•é¡Œï¼š\n{clean_context}\n\n"
        f"ä½¿ç”¨è€…å•é¡Œï¼š{clean_question}\n\n"
        
        "ã€å›ç­”è¦æ±‚ã€‘\n"
        "1. ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œå°ˆæ¥­ã€çµæ§‹åŒ–çš„æ–¹å¼å›ç­”\n"
        "2. æ˜ç¢ºå¼•ç”¨ç›¸é—œ ESG æ¡†æ¶èˆ‡æ¢æ–‡ç·¨è™Ÿï¼ˆè‹¥é©ç”¨ï¼‰\n"
        "3. æä¾›å…·é«”ã€å¯åŸ·è¡Œçš„å»ºè­°\n\n"
        
        "ã€é‡é»æ¡†æ¶å°ç…§ã€‘\n"
        "åœ¨å›ç­”æ™‚ï¼Œè«‹ä¸»å‹•è­˜åˆ¥ä¸¦å¼•ç”¨ä»¥ä¸‹ç›¸é—œæ¡†æ¶æ¨™æº–ï¼š\n"
        "- **IFRS S1/S2**ï¼šæ°¸çºŒæ­éœ²æº–å‰‡ï¼ˆä¸€èˆ¬è¦æ±‚èˆ‡æ°£å€™ç›¸é—œæ­éœ²ï¼‰\n"
        "- **GRI Standards 2021**ï¼šå…¨çƒå ±å‘Šå€¡è­°çµ„ç¹”æ¨™æº–\n"
        "- **SASB (EM-CM)**ï¼šæ°¸çºŒæœƒè¨ˆæº–å‰‡ï¼ˆå»ºæç”¢æ¥­ï¼‰\n"
        "- **TCFD**ï¼šæ°£å€™ç›¸é—œè²¡å‹™æ­éœ²\n"
        "- **EU CBAM**ï¼šæ­ç›Ÿç¢³é‚Šå¢ƒèª¿æ•´æ©Ÿåˆ¶\n"
        "- **MSCI ESG**ï¼šMSCI ESG è©•ç­‰æ–¹æ³•è«–\n\n"
        
        "ã€å›ç­”æ ¼å¼ã€‘\n"
        "è«‹æŒ‰ç…§ä»¥ä¸‹çµæ§‹çµ„ç¹”ä½ çš„å›ç­”ï¼š\n"
        "1ï¸âƒ£ **ç›´æ¥å›ç­”**\n"
        "   - ç°¡æ½”å›æ‡‰æ ¸å¿ƒå•é¡Œ\n\n"
        "2ï¸âƒ£ **æ¡†æ¶å¼•ç”¨**ï¼ˆè‹¥é©ç”¨ï¼‰\n"
        "   - ä¾‹å¦‚ï¼šæ ¹æ“š IFRS S2 ç¬¬ 14 æ¢...\n"
        "   - ä¾‹å¦‚ï¼šç¬¦åˆ GRI 305-1ï¼ˆç›´æ¥æº«å®¤æ°£é«”æ’æ”¾ï¼‰...\n"
        "   - ä¾‹å¦‚ï¼šä¾æ“š SASB EM-CM-110a.1ï¼ˆèƒ½æºç®¡ç†ï¼‰...\n\n"
        "3ï¸âƒ£ **TCC å…·é«”å»ºè­°**\n"
        "   - é‡å°å°æ³¥é›†åœ˜çš„å¯¦å‹™å»ºè­°\n\n"
        "4ï¸âƒ£ **åƒè€ƒä¾æ“š**\n"
        "   - åˆ—å‡ºé—œéµåƒè€ƒä¾†æºæª”æ¡ˆæˆ–æ®µè½\n"
    )


def generate_with_gemini(api_key: str, prompt: str) -> str:
    if genai is None:
        raise RuntimeError("æ‰¾ä¸åˆ° google-generativeai å¥—ä»¶ã€‚è«‹ç¢ºèªå·²å®‰è£ requirements.txt å…§çš„ä¾è³´ã€‚")

    genai.configure(api_key=api_key)

    # å…ˆæŸ¥å‡ºç›®å‰é€™å€‹ API Key å¯ç”¨çš„æ¨¡å‹
    try:
        available_models = list(genai.list_models())
    except Exception as e:  # pragma: no cover
        raise RuntimeError(
            "Gemini å‘¼å«å¤±æ•—ï¼šç„¡æ³•å–å¾—å¯ç”¨æ¨¡å‹æ¸…å–®ï¼Œè«‹ç¢ºèªï¼š\n"
            "1. é€™æ”¯ API Key æ˜¯åœ¨ Google AI Studio ç”¢ç”Ÿçš„ï¼Œè€Œä¸æ˜¯ Google Cloud / å…¶ä»–æœå‹™ã€‚\n"
            "2. å·²åœ¨å°æ‡‰çš„å¸³è™Ÿå•Ÿç”¨ Gemini APIï¼Œä¸”ç¶²è·¯å¯ä»¥é€£åˆ° Googleã€‚\n"
            f"è©³ç´°éŒ¯èª¤ï¼š{e}"
        ) from e

    text_models = [
        m
        for m in available_models
        if hasattr(m, "supported_generation_methods")
        and "generateContent" in getattr(m, "supported_generation_methods", [])
    ]

    if not text_models:
        raise RuntimeError(
            "Gemini å‘¼å«å¤±æ•—ï¼šé€™æ”¯ API Key åä¸‹ç›®å‰æ²’æœ‰æ”¯æ´ generateContent çš„æ¨¡å‹å¯ç”¨ã€‚\n"
            "è«‹åˆ° Google AI Studio çš„ Models / API é é¢ï¼Œç¢ºèªå¸³è™Ÿå·²é–‹é€š Gemini 1.5ï¼ˆä¾‹å¦‚ gemini-1.5-flashï¼‰å¾Œå†è©¦ä¸€æ¬¡ã€‚"
        )

    # å„ªå…ˆæŒ‘é¸åç¨±ä¸­åŒ…å« 1.5 çš„æ¨¡å‹ï¼Œå…¶æ¬¡ä»»æ„å¯ç”¨æ¨¡å‹
    preferred = [m for m in text_models if "1.5" in getattr(m, "name", "")]
    chosen = (preferred or text_models)[0]
    model_name = getattr(chosen, "name", "gemini-1.5-flash")

    try:
        model = genai.GenerativeModel(model_name)
        # åœ¨ç™¼é€å‰å†æ¬¡ç¢ºä¿ prompt æ˜¯ä¹¾æ·¨çš„ UTF-8
        clean_prompt = sanitize_text(prompt)
        resp = model.generate_content(clean_prompt)
    except Exception as e:  # pragma: no cover
        raise RuntimeError(
            "Gemini å‘¼å«å¤±æ•—ï¼šé›–ç„¶æˆåŠŸæ‰¾åˆ°å¯ç”¨æ¨¡å‹ "
            f"`{model_name}`ï¼Œä½†åœ¨å‘¼å« generateContent æ™‚å‡ºç¾éŒ¯èª¤ï¼š{e}\n"
            "è«‹åˆ° Google AI Studio æ¸¬è©¦åŒä¸€æ”¯ API Key æ˜¯å¦å¯ä»¥æ­£å¸¸å‘¼å«ç›¸åŒæ¨¡å‹ã€‚"
        ) from e

    text = getattr(resp, "text", None)
    if not text:
        # Fallback for some response shapes
        try:
            text = resp.candidates[0].content.parts[0].text  # type: ignore[attr-defined]
        except Exception:
            text = ""

    if not text.strip():
        raise RuntimeError("Gemini æœªå›å‚³æœ‰æ•ˆæ–‡å­—å…§å®¹ï¼Œè«‹ç¨å¾Œå†è©¦æˆ–ç¢ºèª API Key/é…é¡ã€‚")

    return text.strip()


def main() -> None:
    st.set_page_config(page_title="TCC ESG Intelligent Knowledge Base", layout="wide")

    st.title("ğŸŒ¿ å°æ³¥ (TCC) ä¼æ¥­ ESG æ™ºèƒ½çŸ¥è­˜åº«")
    st.caption("Powered by Google Gemini 1.5 Pro & MSCI Methodology")

    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š (Settings)")
        api_key = st.text_input("Google Gemini API Key", type="password", value="")
        uploaded_files = st.file_uploader(
            "ä¸Šå‚³ ESG çŸ¥è­˜åº«æª”æ¡ˆï¼ˆå¯å¤šé¸ï¼šCSV / Excel / PDF / Word / PPTï¼‰",
            type=["csv", "xlsx", "pdf", "docx", "pptx"],
            accept_multiple_files=True,
        )

    df, kb_err = get_kb_dataframe(uploaded_files)
    
    # Metadata filtering UI (after loading KB)
    with st.sidebar:
        if df is not None and not kb_err:
            st.divider()
            st.header("ğŸ“… è³‡æ–™ç¯©é¸ (Filters)")
            
            # Year filter
            if 'year' in df.columns:
                available_years = sorted([int(y) for y in df['year'].dropna().unique()])
                if available_years:
                    selected_years = st.multiselect(
                        "ğŸ—“ï¸ æ–‡ä»¶å¹´ä»½",
                        options=available_years,
                        default=[],
                        help="é¸æ“‡ç‰¹å®šå¹´ä»½çš„æ–‡ä»¶ï¼ˆç•™ç©º=å…¨éƒ¨ï¼‰"
                    )
                    
                    # Apply year filter
                    if selected_years:
                        df = df[df['year'].isin(selected_years)]
                        st.success(f"âœ“ å·²ç¯©é¸ {len(selected_years)} å€‹å¹´ä»½")
            
            # Document type filter
            if 'doc_type' in df.columns:
                available_types = sorted(df['doc_type'].dropna().unique())
                if available_types:
                    selected_doc_types = st.multiselect(
                        "ğŸ“‚ æ–‡ä»¶é¡å‹",
                        options=available_types,
                        default=[],
                        help="é¸æ“‡ç‰¹å®šé¡å‹çš„æ–‡ä»¶ï¼ˆç•™ç©º=å…¨éƒ¨ï¼‰"
                    )
                    
                    # Apply doc type filter
                    if selected_doc_types:
                        df = df[df['doc_type'].isin(selected_doc_types)]
                        st.success(f"âœ“ å·²ç¯©é¸ {len(selected_doc_types)} ç¨®é¡å‹")
    if kb_err:
        st.error(kb_err)

    if not api_key.strip():
        st.info("è«‹å…ˆåœ¨å´é‚Šæ¬„è¼¸å…¥ **Google Gemini API Key** æ‰èƒ½é–‹å§‹åˆ†æã€‚")

    question = st.text_area(
        "è«‹è¼¸å…¥ä½ çš„å•é¡Œ",
        placeholder="ä¾‹å¦‚ï¼šMSCI å°æ–¼ç’°å¢ƒç®¡ç†ï¼ˆEï¼‰åœ¨è©•ç­‰æ–¹æ³•è«–ä¸­é€šå¸¸æœƒå¦‚ä½•è¡¡é‡ï¼Ÿ",
        disabled=not api_key.strip(),
    )

    run = st.button("é–‹å§‹åˆ†æ", type="primary", disabled=(not api_key.strip() or not question.strip() or df is None))

    if run:
        if df is None:
            st.error("çŸ¥è­˜åº«å°šæœªå°±ç·’ï¼šè«‹ä¸Šå‚³æœ‰æ•ˆ CSV æˆ–æ”¾ç½®é è¨­æª”æ¡ˆæ–¼åŒç›®éŒ„ã€‚")
            return

        # åˆå§‹åŒ–å‘é‡è³‡æ–™åº«ï¼ˆä½¿ç”¨ cacheï¼‰
        with st.spinner("æ­£åœ¨åˆå§‹åŒ–å‘é‡è³‡æ–™åº«..."):
            collection = init_vector_db(df, api_key.strip())
        
        # æ··åˆæª¢ç´¢
        with st.spinner("æ­£åœ¨æª¢ç´¢ç›¸é—œæ®µè½ï¼ˆæ··åˆèªç¾©+é—œéµå­—ï¼‰..."):
            chunks = hybrid_retrieve_topk(collection, df, question, k=5)

        if not chunks:
            st.warning("æ‰¾ä¸åˆ°èˆ‡å•é¡Œç›¸é—œçš„æ®µè½ï¼ˆä»¥ç°¡æ˜“å­—ä¸²æ¯”å°ï¼‰ã€‚ä½ å¯ä»¥å˜—è©¦æ›å€‹èªªæ³•ã€åŠ å…¥æ›´å¤šé—œéµå­—ï¼Œæˆ–ç¢ºèª CSV çš„ `text_content` å…§å®¹ã€‚")
            return

        context = "\n\n---\n\n".join([c.text_content for c in chunks])
        prompt = build_prompt(context=context, question=question.strip())

        try:
            with st.spinner("æ­£åœ¨å‘¼å« Gemini 1.5 Pro ç”Ÿæˆå›ç­” ..."):
                answer = generate_with_gemini(api_key.strip(), prompt)
        except Exception as e:
            st.error(str(e))
            return

        st.subheader("AI å›ç­”")
        st.write(answer)

        with st.expander("ğŸ“š æŸ¥çœ‹åƒè€ƒä¾†æº / References"):
            for i, c in enumerate(chunks, start=1):
                # æ§‹å»º metadata é¡¯ç¤º
                page_info = f"ç¬¬ {c.page_number} é " if c.page_number else ""
                year_info = f"{c.year} å¹´" if c.year else "N/A"
                
                st.markdown(
                    f"**#{i}** ğŸ“„ `{c.source_file}` {f'({page_info})' if page_info else ''}\n\n"
                    f"é¡å‹ï¼š`{c.doc_type}` ï½œå¹´ä»½ï¼š`{year_info}` ï½œç›¸é—œåº¦ï¼š`{c.score}%`"
                )
                st.write(c.text_content)
                st.divider()


if __name__ == "__main__":
    main()

